# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4TAopBpDrmVBjNNJlP9c9bYMqV23EPt
"""

import torch
import numpy as np
import math
import joblib
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForMaskedLM
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

def load_stereoset_all():
    ds_intra = load_dataset("McGill-NLP/stereoset", "intrasentence", split="validation")
    ds_inter = load_dataset("McGill-NLP/stereoset", "intersentence", split="validation")
    return DatasetDict({
        "intrasentence": ds_intra,
        "intersentence": ds_inter
    })

ds = load_stereoset_all()
print(ds)

GENDER_PAIRS = {
    "he": "she", "she": "he",
    "him": "her", "her": "him",
    "his": "hers", "hers": "his",
    "man": "woman", "woman": "man",
    "men": "women", "women": "men",
    "boy": "girl", "girl": "boy",
    "boys": "girls", "girls": "boys",
}

def cda_swap(sentence: str):
    words = sentence.split()
    swapped = []
    for w in words:
        low = w.lower()
        if low in GENDER_PAIRS:
            new = GENDER_PAIRS[low]
            swapped.append(new if w.islower() else new.capitalize())
        else:
            swapped.append(w)
    return " ".join(swapped)

def extract_S_A_U(ex):
    if isinstance(ex["sentences"], dict):  # intrasentence
        s = a = u = None
        for sent, label in zip(ex["sentences"]["sentence"], ex["sentences"]["gold_label"]):
            if label == 0: s = sent
            elif label == 1: a = sent
            elif label == 2: u = sent
        return s, a, u

    if isinstance(ex["sentences"], list):  # intersentence
        s = a = u = None
        for e in ex["sentences"]:
            if e["label"] == "stereotype": s = e["sentence"]
            elif e["label"] == "anti-stereotype": a = e["sentence"]
            elif e["label"] == "unrelated": u = e["sentence"]
        return s, a, u

    return None, None, None

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased").to(device)
model.eval()

def pll(sentence):
    if not sentence: return float("-inf")

    enc = tokenizer(sentence, return_tensors="pt", truncation=True)
    ids = enc["input_ids"][0].to(device)
    mask = enc["attention_mask"][0].to(device)

    total = 0.0
    count = 0

    for i in range(1, len(ids)-1):
        if mask[i] == 0: continue
        orig = ids[i].item()
        if orig in [tokenizer.cls_token_id, tokenizer.sep_token_id]: continue

        masked = ids.clone()
        masked[i] = tokenizer.mask_token_id

        with torch.no_grad():
            out = model(masked.unsqueeze(0), attention_mask=mask.unsqueeze(0)).logits

        log_pro = torch.nn.functional.log_softmax(out[0, i], dim=-1)[orig]
        total += log_pro.item()
        count += 1

    return total if count > 0 else float("-inf")

def stereoset_scores(subset):
    PLL_S, PLL_A, PLL_U = [], [], []

    for ex in subset:
        s, a, u = extract_S_A_U(ex)
        if s: PLL_S.append(pll(s))
        if a: PLL_A.append(pll(a))
        if u: PLL_U.append(pll(u))

    μS, μA = np.mean(PLL_S), np.mean(PLL_A)
    μU = np.mean(PLL_U)
    μM = np.mean([μS, μA])

    SSS = abs((μS/(μS+μA)) - 0.5) * 2
    LMS = 1 - (μU / μM)
    ICAT = (1 - SSS) * LMS

    return {
        "SSS": round(SSS * 100, 2),
        "LMS": round(LMS * 100, 2),
        "ICAT": round(ICAT * 100, 2),
        "mu_S": μS, "mu_A": μA, "mu_U": μU
    }

def stereoset_scores_cda(subset):
    PLL_S, PLL_A, PLL_U = [], [], []

    for ex in subset:
        s, a, u = extract_S_A_U(ex)
        s2 = cda_swap(s) if s else None
        a2 = cda_swap(a) if a else None
        u2 = cda_swap(u) if u else None

        if s2: PLL_S.append(pll(s2))
        if a2: PLL_A.append(pll(a2))
        if u2: PLL_U.append(pll(u2))

    μS, μA = np.mean(PLL_S), np.mean(PLL_A)
    μU = np.mean(PLL_U)
    μM = np.mean([μS, μA])

    SSS = abs((μS/(μS+μA)) - 0.5) * 2
    LMS = 1 - (μU / μM)
    ICAT = (1 - SSS) * LMS

    return {
        "SSS": round(SSS * 100, 2),
        "LMS": round(LMS * 100, 2),
        "ICAT": round(ICAT * 100, 2)
    }

print("=== CDA RESULTS ===")
cda_intra = stereoset_scores_cda(ds["intrasentence"])
cda_inter = stereoset_scores_cda(ds["intersentence"])
cda_intra, cda_inter

PRONOUNS = ["he", "she", "him", "her", "his", "hers"]

def collect_inlp_embeddings():
    X = []
    y = []
    for ex in ds["intrasentence"]:
        for p in PRONOUNS:
            if p in ex["context"].lower():
                emb = model.bert(
                    **tokenizer(ex["context"], return_tensors="pt").to(device)
                ).last_hidden_state[:,0,:].detach().cpu().numpy()
                X.append(emb)
                y.append(0 if p in ["he","him","his"] else 1)
    return np.vstack(X), np.array(y)

X, y = collect_inlp_embeddings()
print(X.shape, y.shape)

def learn_inlp_projection(X, y, k=10):
    P = np.eye(X.shape[1])
    for i in range(k):
        clf = SGDClassifier(max_iter=2000).fit(X @ P, y)
        w = clf.coef_[0]
        w = w / np.linalg.norm(w)
        P = P @ (np.eye(len(w)) - np.outer(w, w))
    return P

P = learn_inlp_projection(X, y, k=10)
P = torch.tensor(P, dtype=torch.float32).to(device)
print("INLP projection shape:", P.shape)

def pll_inlp(sentence):
    enc = tokenizer(sentence, return_tensors="pt", truncation=True)
    ids = enc["input_ids"][0].to(device)
    mask = enc["attention_mask"][0].to(device)
    outputs = model.bert(ids.unsqueeze(0), attention_mask=mask.unsqueeze(0)).last_hidden_state

    # Apply P to hidden states
    hidden = outputs @ P

    total, count = 0, 0
    for i in range(1, len(ids)-1):
        orig = ids[i].item()
        masked = ids.clone()
        masked[i] = tokenizer.mask_token_id
        with torch.no_grad():
            out = model(masked.unsqueeze(0), attention_mask=mask.unsqueeze(0)).logits
        log_probs = torch.nn.functional.log_softmax(out[0, i], dim=-1)[orig]
        total += log_probs.item()
        count += 1

    return total / max(count,1)

def stereoset_scores_inlp_cda(subset):
    PLL_S = []; PLL_A = []; PLL_U = []
    for ex in subset:
        s, a, u = extract_S_A_U(ex)
        if s: PLL_S.append(pll_inlp(cda_swap(s)))
        if a: PLL_A.append(pll_inlp(cda_swap(a)))
        if u: PLL_U.append(pll_inlp(cda_swap(u)))

    μS, μA = np.mean(PLL_S), np.mean(PLL_A)
    μU = np.mean(PLL_U)
    μM = np.mean([μS, μA])

    SSS = abs((μS/(μS+μA)) - 0.5) * 2
    LMS = 1 - (μU / μM)
    ICAT = (1 - SSS) * LMS

    return {
        "SSS": round(SSS*100,2),
        "LMS": round(LMS*100,2),
        "ICAT": round(ICAT*100,2)
    }

print("=== INLP + CDA RESULTS ===")
inlp_cda_intra = stereoset_scores_inlp_cda(ds["intrasentence"])
inlp_cda_inter = stereoset_scores_inlp_cda(ds["intersentence"])
inlp_cda_intra, inlp_cda_inter